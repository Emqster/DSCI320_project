{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Putting libraries in this code chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for DSCI 320, Fall 2024\n",
    "\n",
    "In this project we will\n",
    "\n",
    "1) Implement the very basic version of support vector machine.\n",
    "2) Apply support vector machine to do a classification problem.\n",
    "3) Apply neural network to do the same classification problem.\n",
    "\n",
    "We have three classification problems for you to choose. The first is the identification of fake news, with dataset available at:\n",
    "\n",
    "https://www.kaggle.com/competitions/fake-news/data (this is the one we're doing)\n",
    "\n",
    "Dataset information:\n",
    "\n",
    "train.csv: A full training dataset with the following attributes:\n",
    "- id: unique id for a news article\n",
    "- title: the title of a news article\n",
    "- author: author of the news article\n",
    "- text: the text of the article; could be incomplete\n",
    "- label: a label that marks the article as potentially unreliable (1: unreliable, 0: reliable)\n",
    "\n",
    "test.csv: A testing training dataset with all the same attributes at train.csv without the label.\n",
    "\n",
    "submit.csv: A sample submission that you can\n",
    "\n",
    "For any chosen problem, data normalization will always be helpful. Those who understand the singular value decomposition (SVD) well could try to use SVD first to reduce the dimensionality before moving to the classification using SVM or neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Naıve implementation of support vector machine\n",
    "\n",
    "Here we will create the a small 2-D dataset and implement a naıve algorithm to find the hyperplane that separate the two clusters in the dataset. Let the hyperplane be $y = w^{T}x$ where $x$ is the extended variable padded with 1s, the algorithm goes as:\n",
    "\n",
    "A simple way to generate small datasets is using make_blobs in sklearn. This way one can generate points of a given number in 2-D plane clustered around given centers, for example:\n",
    "\n",
    "        from sklearn.datasets import make_blobs\n",
    "        centers = [(1,1),(-1,-1])] # centers of the two clusters\n",
    "        cluster_std = [0.2,0.2] # variance of the two clusters\n",
    "        X, Y=make_blobs(n_samples=10, cluster_std=cluster_std, centers=centers,\n",
    "                        n_features=2, random_state=1) # two clusters, 10 points in total.\n",
    "\n",
    "Please note that the labels of the two clusters will be 0 and 1 in this case, and you will have to change them to be 1 and −1, so they are consistent with the algorithm. For this part, please make a few figures showing the improved separation of clusters as the iterations continue and the final state of the separation. Attach these figures to your project report.\n",
    "\n",
    "1) Define learning rate $l_{r} = 0.1$\n",
    "2) Define expand factor $f_{e} = 0.9$\n",
    "3) Define reduce factor $f_{r} = 1.1$\n",
    "4) Pick an arbitrary data point $(x, y)$ and determine whether it is misclassified\n",
    "\n",
    "        if Classified correctly then\n",
    "                if Margin too small then\n",
    "                        w ← w + lr · fr · yx\n",
    "                else if Margin too wide then\n",
    "                        w ← fe · w\n",
    "                end if\n",
    "        else\n",
    "                w ← w + lr · yx\n",
    "        end if\n",
    "                Goto 4 \n",
    "        \n",
    "and continue the process until convergence, or a preset number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## code chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classification using support vector machine\n",
    "\n",
    "In this step we will implement the support vector machine modules in sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) which also allows you to use various type of kernels. You can then compare results with and without kernels. You are expected to:\n",
    "\n",
    "1) Load and understand the dataset.\n",
    "2) Implement SVM in sklearn for the multi-class classification.\n",
    "3) Understand the functionality of various kernels for SVM and compare their performance for the problem. Quantify the accuracy of your classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## code chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification Using Neural Network\n",
    "\n",
    "In this step you will implement neural network for classification. You are not required to build your own neural network and its training. Instead, you may use the Multilayer Perception Classifier in sklearn. More details information about the classification function can be found at https://scikit-learn.org/stable/modules/neural_networks_supervised.html and https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html. Alternatively, you could work with PyTorch (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). In either case, we might want to walk through the sample given in these introductory pages and make sure they are pruning as expected. Then read the sample code very carefully, understand each line and every single feature or parameter of the method, before moving to build our own implementation and applications. These features or parameter include:\n",
    "\n",
    "1) Depth and width of the network.\n",
    "2) Solver of the minimization problem.\n",
    "3) Target function and strength of regularization\n",
    "4) Batch size.\n",
    "5) Initial and adaptive earning rate.\n",
    "6) Initialization of the network.\n",
    "7) Momentum method.\n",
    "8) Termination of the training.\n",
    "9) Use of well-trained network for prediction on test dataset.\n",
    "\n",
    "Your datasets contain both train and test sets, so you will be able to quantify your classification.\n",
    "\n",
    "At the end of these three parts, each group is expected to:\n",
    "\n",
    "1) Describe the problem and your algorithm.\n",
    "2) Describe your implementation of the algorithm, major steps, and key parameters.\n",
    "3) Describe the training process. Quantify the accuracy of your classifications.\n",
    "4) Wrap up the results and finish a project report with eight or more pages (excluding your code), including diagrams, tables, or figures.\n",
    "5) Python code will be submitted separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## code chunk"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
