{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "''' Libraries here '''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "''' Loading in our data '''\n",
    "test_df = pd.read_csv('/content/test.csv')\n",
    "train_df = pd.read_csv('/content/train.csv')\n",
    "sample_submission_data = pd.read_csv('/content/submit.csv')\n",
    "\n",
    "''' ctrl + / for mass comment on codes, can view and hide comment column '''\n",
    "# print(train_df.head(5)) # id, title, author, text, label (1: unreliable, 0: reliable)\n",
    "# print('~~~~~~~~~~~~')\n",
    "# print(sample_submission_data.head(5)) # id, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for DSCI 320, Fall 2024\n",
    "\n",
    "In this project we will\n",
    "\n",
    "1) Implement the very basic version of support vector machine.\n",
    "2) Apply support vector machine to do a classification problem.\n",
    "3) Apply neural network to do the same classification problem.\n",
    "\n",
    "We have three classification problems for you to choose. The first is the identification of fake news, with dataset available at:\n",
    "\n",
    "https://www.kaggle.com/competitions/fake-news/data (this is the one we're doing)\n",
    "\n",
    "Dataset information:\n",
    "\n",
    "train.csv: A full training dataset with the following attributes:\n",
    "- id: unique id for a news article\n",
    "- title: the title of a news article\n",
    "- author: author of the news article\n",
    "- text: the text of the article; could be incomplete\n",
    "- label: a label that marks the article as potentially unreliable (1: unreliable, 0: reliable)\n",
    "\n",
    "test.csv: A testing training dataset with all the same attributes at train.csv without the label.\n",
    "\n",
    "submit.csv: A sample submission that you can\n",
    "\n",
    "For any chosen problem, data normalization will always be helpful. Those who understand the singular value decomposition (SVD) well could try to use SVD first to reduce the dimensionality before moving to the classification using SVM or neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Naıve implementation of support vector machine\n",
    "\n",
    "Here we will create the a small 2-D dataset and implement a naıve algorithm to find the hyperplane that separate the two clusters in the dataset. Let the hyperplane be $y = w^{T}x$ where $x$ is the extended variable padded with 1s, the algorithm goes as:\n",
    "\n",
    "A simple way to generate small datasets is using make_blobs in sklearn. This way one can generate points of a given number in 2-D plane clustered around given centers, for example:\n",
    "\n",
    "        from sklearn.datasets import make_blobs\n",
    "        centers = [(1,1),(-1,-1])] # centers of the two clusters\n",
    "        cluster_std = [0.2,0.2] # variance of the two clusters\n",
    "        X, Y=make_blobs(n_samples=10, cluster_std=cluster_std, centers=centers,\n",
    "                        n_features=2, random_state=1) # two clusters, 10 points in total.\n",
    "\n",
    "Please note that the labels of the two clusters will be 0 and 1 in this case, and you will have to change them to be 1 and −1, so they are consistent with the algorithm. For this part, please make a few figures showing the improved separation of clusters as the iterations continue and the final state of the separation. Attach these figures to your project report.\n",
    "\n",
    "1) Define learning rate $l_{r} = 0.1$\n",
    "2) Define expand factor $f_{e} = 0.9$\n",
    "3) Define reduce factor $f_{r} = 1.1$\n",
    "4) Pick an arbitrary data point $(x, y)$ and determine whether it is misclassified\n",
    "\n",
    "        if Classified correctly then\n",
    "                if Margin too small then\n",
    "                        w ← w + lr · fr · yx\n",
    "                else if Margin too wide then\n",
    "                        w ← fe · w\n",
    "                end if\n",
    "        else\n",
    "                w ← w + lr · yx\n",
    "        end if\n",
    "                Goto 4 \n",
    "        \n",
    "and continue the process until convergence, or a preset number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want to implement a basic version of an SVM algorithm on a simple 2D dataset (???)\n",
    "I don't think we're using our data sets here, just making a synthetic one...\n",
    "need clusters, centers, X,Y, hyperplane. Use make_blobs in sklearn\n",
    "\"\"\"\n",
    "centers = [(1,1), (-1,-1)] # centers of the two clusters\n",
    "cluster_std = [0.2, 0.2] # variance (SD) of the two clusters\n",
    "X, Y = make_blobs(n_samples=10, cluster_std=cluster_std, centers=centers, n_features=2, random_state=1) # two clusters, 10 points in total.\n",
    "\n",
    "Y = np.where(Y == 0, -1, 1) # \"you will have to change them to be 1 and −1\", np.where(condition, x, y)\n",
    "\n",
    "# print(X)\n",
    "# print('~~~~~~~~~')\n",
    "# print(Y)\n",
    "# print('~~~~~~~~~')\n",
    "# plt.scatter(X[:, 0], X[:, 1], c = Y) # plotting x and y on scatter plot to observe\n",
    "# plt.show()\n",
    "\n",
    "X = np.hstack([X, np.ones((X.shape[0], 1))]) # \"x  is the extended variable padded with 1s\", bias term\n",
    "# print(X) # new column of 1 values the length of X\n",
    "\n",
    "''' y = (w^T)x, w is the vector of weights (slope), x is the data point, y is the label... I think'''\n",
    "w = np.random.randn(3) # 3 for columns + bias term\n",
    "\n",
    "def func(X, Y, w): # here's the function for our hyperplane\n",
    "  hyperplane = Y * np.dot(w, X)\n",
    "  return hyperplane\n",
    "\n",
    "lr = 0.1 # learning rate\n",
    "fe = 0.9 # expand factor\n",
    "fr = 1.1 # reduce factor\n",
    "nmax = 100 # max iterations, I wasn't sure if this should be 100 or 1000\n",
    "\n",
    "def svm(X, Y, w, lr, fe, fr, nmax):\n",
    "\n",
    "  point_classification = np.zeros(len(X), dtype=bool) # seeing if the point is correctly classified at the end of loops\n",
    "\n",
    "  for i in range(nmax):\n",
    "    for j in range(len(X)):\n",
    "      current_X = X[j]\n",
    "      current_Y = Y[j]\n",
    "      margin = func(current_X, current_Y, w) # set equal to hyperplane functions above\n",
    "      if (margin > 0): # \"if classified correctly then\"\n",
    "        point_classification[j] = True # point was correctly classified\n",
    "        if (margin < 1): # \"if Margin too small then\"\n",
    "          w = w + (lr * fr * current_Y * current_X) # \"w ← w + lr · fr · yx\"\n",
    "        elif (margin > 1): # \"else if Margin too wide then\"\n",
    "          w = fe * w # \"w ← fe · w\"\n",
    "      else:\n",
    "        w = w * (lr * current_Y * current_X) # \"w ← w + lr · yx\"\n",
    "        point_classification[j] = False # point was misclasified\n",
    "\n",
    "  return w, point_classification # return the final weight vector and the final classification\n",
    "\n",
    "''' Pick an arbitrary data point (x,y) and determine whether it is misclassified '''\n",
    "test_point_w, test_classification = svm(X, Y, w, lr, fe, fr, nmax)\n",
    "\n",
    "for l, classified in enumerate(test_classification): # I want to test all ten points instead of one for comparison\n",
    "  current_point = X[l, :-1]\n",
    "  point_class = Y[1]\n",
    "  if classified:\n",
    "      print(f\"Our arbitrary data point {l} (X = {current_point}, Y = {point_class}) is classified correctly.\")\n",
    "  else:\n",
    "      print(f\"Our arbitrary data point {l} (X = {current_point}, Y = {point_class}) is misclassified.\")\n",
    "\n",
    "correct_points = X[test_classification == True]\n",
    "misclassified_points = X[test_classification == False]\n",
    "plt.scatter(correct_points[:, 0], correct_points[:, 1], c = 'blue', label = \"Correctly Classified\")\n",
    "plt.scatter(misclassified_points[:, 0], misclassified_points[:, 1], c = 'red', label = \"Misclassified\")\n",
    "x_vals = np.linspace(-3, 3, 100)\n",
    "y_vals = (-test_point_w[0] * x_vals - test_point_w[2]) / test_point_w[1]  # Compute corresponding y values for the hyperplane equation\n",
    "plt.plot(x_vals, y_vals, color='green', label='Hyperplane')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('SVM Calculate Hyperplane and Point Classifications')\n",
    "plt.legend()\n",
    "plt.grid(alpha = 0.5) # making the grid lines a little transparent bc I think it looks nicer :)\n",
    "\n",
    "'''\n",
    "I'm guessing that there's some misclassified points because the testing data is so small.\n",
    "Everytime you run this code new points and calculations are generated, if we put this on our report\n",
    "  then I vote we choose a graph that generates more classified points lol\n",
    "'''\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classification using support vector machine\n",
    "\n",
    "In this step we will implement the support vector machine modules in sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) which also allows you to use various type of kernels. You can then compare results with and without kernels. You are expected to:\n",
    "\n",
    "1) Load and understand the dataset.\n",
    "2) Implement SVM in sklearn for the multi-class classification.\n",
    "3) Understand the functionality of various kernels for SVM and compare their performance for the problem. Quantify the accuracy of your classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## code chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification Using Neural Network\n",
    "\n",
    "In this step you will implement neural network for classification. You are not required to build your own neural network and its training. Instead, you may use the Multilayer Perception Classifier in sklearn. More details information about the classification function can be found at https://scikit-learn.org/stable/modules/neural_networks_supervised.html and https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html. Alternatively, you could work with PyTorch (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). In either case, we might want to walk through the sample given in these introductory pages and make sure they are pruning as expected. Then read the sample code very carefully, understand each line and every single feature or parameter of the method, before moving to build our own implementation and applications. These features or parameter include:\n",
    "\n",
    "1) Depth and width of the network.\n",
    "2) Solver of the minimization problem.\n",
    "3) Target function and strength of regularization\n",
    "4) Batch size.\n",
    "5) Initial and adaptive earning rate.\n",
    "6) Initialization of the network.\n",
    "7) Momentum method.\n",
    "8) Termination of the training.\n",
    "9) Use of well-trained network for prediction on test dataset.\n",
    "\n",
    "Your datasets contain both train and test sets, so you will be able to quantify your classification.\n",
    "\n",
    "At the end of these three parts, each group is expected to:\n",
    "\n",
    "1) Describe the problem and your algorithm.\n",
    "2) Describe your implementation of the algorithm, major steps, and key parameters.\n",
    "3) Describe the training process. Quantify the accuracy of your classifications.\n",
    "4) Wrap up the results and finish a project report with eight or more pages (excluding your code), including diagrams, tables, or figures.\n",
    "5) Python code will be submitted separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## code chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caitlyn's test chunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
